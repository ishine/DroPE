defaults:
  - override /trainer_cfg@_global_: base_trainer
  - override /model_cfg@_global_: smollm/smollm1_7b
  - override /data_cfg@_global_: fineweb_edu_shuffled
  - _self_

custom_class: custom_models.drope.DroPELlamaForCausalLM
from_pretrained: true
drope: true

model_custom_config:
  _target_: custom_models.drope.DroPELlamaConfig.from_pretrained
  pretrained_model_name_or_path: ${model_name_or_path}
  attention_type: nope


max_seq_length: 2048
max_steps: 120000
learning_rate: 3e-4

wandb_project: drope
wandb_run_name: ${trainer_log_name}_${model_log_name}_${max_steps}steps_lr${learning_rate}

per_device_train_batch_size: 64

save_steps: 100

lr_scheduler_type: warmup_stable_decay

lr_scheduler_kwargs:
  # num_warmup_steps: 600
  num_stable_steps: 96000
  num_decay_steps: 23400

  warmup_type: linear
  decay_type: cosine
  num_cycles: 0.5

  min_lr_ratio: 0.1

trainer_args:
  warmup_steps: 600
